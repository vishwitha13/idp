# -*- coding: utf-8 -*-
"""Heart

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dXNK4ZdjXFwo76uS3tt6cyvBsOX0mOGK
"""

import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics import accuracy_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder  # Import OneHotEncoder
from sklearn.model_selection import StratifiedKFold
from sklearn.compose import ColumnTransformer  # Import ColumnTransformer
import pickle

# Load dataset from a CSV file (replace 'heart.csv' with the actual file path)
df = pd.read_csv('HeartDiseaseTrain-Test.csv')

# Replace target values based on your logic (if target values need encoding)
df['target'] = df['target'].replace({0: 0, 1: 1, 2: 1, 3: 1, 4: 1})

# Separate features (X) and target (y)
X = df.drop(columns=['target'])  # Assuming 'target' is the column name for the labels
y = df['target']

# Initialize Stratified K-Fold
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Convert y to a numpy array for indexing
y_np = y.to_numpy()

# Identify numerical and categorical features
numerical_features = X.select_dtypes(include=np.number).columns.tolist()
categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()

# Create a ColumnTransformer to handle numerical and categorical features separately
preprocessor = ColumnTransformer(
    transformers=[
        ('num', SimpleImputer(strategy='mean'), numerical_features),
        ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), categorical_features), #sparse=False for numpy array
    ])

# Perform cross-validation
accuracies = []
for train_index, test_index in skf.split(X, y):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y_np[train_index], y_np[test_index]

    # Preprocess the data using the ColumnTransformer
    X_train_preprocessed = preprocessor.fit_transform(X_train)
    X_test_preprocessed = preprocessor.transform(X_test)


    # Scale the numerical features (after imputation and one-hot encoding)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_preprocessed)
    X_test_scaled = scaler.transform(X_test_preprocessed)

    # Initialize KMeans with optimized parameters
    kmeans = KMeans(n_clusters=2, init='k-means++', n_init=50, max_iter=500, random_state=42)

    # Fit KMeans on the scaled training data
    kmeans.fit(X_train_scaled)

    # Predict cluster labels for the testing data
    test_cluster_labels = kmeans.predict(X_test_scaled)

    # Assign labels based on majority class in each cluster
    cluster_0_label = np.bincount(y_test[test_cluster_labels == 0]).argmax()
    cluster_1_label = np.bincount(y_test[test_cluster_labels == 1]).argmax()
    predicted_labels = np.where(test_cluster_labels == 0, cluster_0_label, cluster_1_label)

    # Calculate accuracy
    accuracy = accuracy_score(y_test, predicted_labels)
    accuracies.append(accuracy)

# Average accuracy across folds
avg_accuracy = np.mean(accuracies)
print("Average Cross-Validation Accuracy with KMeans:", avg_accuracy)

import pandas as pd
import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.metrics import accuracy_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import StratifiedKFold
from sklearn.compose import ColumnTransformer
from sklearn.neighbors import NearestNeighbors  # For DBSCAN optimal eps selection

# Load dataset from a CSV file (replace 'heart.csv' with the actual file path)
df = pd.read_csv('HeartDiseaseTrain-Test.csv')

# Replace target values based on your logic (if target values need encoding)
df['target'] = df['target'].replace({0: 0, 1: 1, 2: 1, 3: 1, 4: 1})

# Separate features (X) and target (y)
X = df.drop(columns=['target'])
y = df['target']

# Initialize Stratified K-Fold
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Convert y to a numpy array for indexing
y_np = y.to_numpy()

# Identify numerical and categorical features
numerical_features = X.select_dtypes(include=np.number).columns.tolist()
categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()

# Create a ColumnTransformer to handle numerical and categorical features separately
preprocessor = ColumnTransformer(
    transformers=[
        ('num', SimpleImputer(strategy='mean'), numerical_features),
        ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), categorical_features),
    ])

# Perform cross-validation for DBSCAN
accuracies_dbscan = []
for train_index, test_index in skf.split(X, y):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y_np[train_index], y_np[test_index]

    # Preprocess the data using the ColumnTransformer
    X_train_preprocessed = preprocessor.fit_transform(X_train)
    X_test_preprocessed = preprocessor.transform(X_test)

    # Scale the numerical features (after imputation and one-hot encoding)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_preprocessed)
    X_test_scaled = scaler.transform(X_test_preprocessed)

    # DBSCAN (using optimal eps determined from NearestNeighbors)
    neighbors = NearestNeighbors(n_neighbors=5).fit(X_train_scaled)
    distances, _ = neighbors.kneighbors(X_train_scaled)
    eps = np.percentile(distances[:, -1], 90)  # A heuristic for eps selection
    dbscan = DBSCAN(eps=eps, min_samples=5)

    # Fit DBSCAN on the scaled training data
    dbscan.fit(X_train_scaled)

    # Predict cluster labels for the testing data
    test_cluster_labels = dbscan.fit_predict(X_test_scaled)

    # Assign labels based on majority class in each cluster
    cluster_0_label = np.bincount(y_test[test_cluster_labels == 0]).argmax()
    predicted_labels = np.where(test_cluster_labels == 0, cluster_0_label, -1)  # For noise points (-1)

    # Calculate accuracy
    accuracy = accuracy_score(y_test, predicted_labels)
    accuracies_dbscan.append(accuracy)

# Average accuracy across folds
avg_accuracy_dbscan = np.mean(accuracies_dbscan)
print("Average Cross-Validation Accuracy with DBSCAN:", avg_accuracy_dbscan)

import pandas as pd
import numpy as np
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import accuracy_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import StratifiedKFold
from sklearn.compose import ColumnTransformer

# Load dataset from a CSV file (replace 'heart.csv' with the actual file path)
df = pd.read_csv('HeartDiseaseTrain-Test.csv')

# Replace target values based on your logic (if target values need encoding)
df['target'] = df['target'].replace({0: 0, 1: 1, 2: 1, 3: 1, 4: 1})

# Separate features (X) and target (y)
X = df.drop(columns=['target'])
y = df['target']

# Initialize Stratified K-Fold
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Convert y to a numpy array for indexing
y_np = y.to_numpy()

# Identify numerical and categorical features
numerical_features = X.select_dtypes(include=np.number).columns.tolist()
categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()

# Create a ColumnTransformer to handle numerical and categorical features separately
preprocessor = ColumnTransformer(
    transformers=[
        ('num', SimpleImputer(strategy='mean'), numerical_features),
        ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), categorical_features),
    ])

# Perform cross-validation for Agglomerative Clustering
accuracies_agglomerative = []
for train_index, test_index in skf.split(X, y):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y_np[train_index], y_np[test_index]

    # Preprocess the data using the ColumnTransformer
    X_train_preprocessed = preprocessor.fit_transform(X_train)
    X_test_preprocessed = preprocessor.transform(X_test)

    # Scale the numerical features (after imputation and one-hot encoding)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_preprocessed)
    X_test_scaled = scaler.transform(X_test_preprocessed)

    # Initialize Agglomerative Clustering
    agglomerative = AgglomerativeClustering(n_clusters=2)

    # Fit Agglomerative Clustering on the scaled training data
    agglomerative.fit(X_train_scaled)

    # Predict cluster labels for the testing data
    test_cluster_labels = agglomerative.fit_predict(X_test_scaled)

    # Assign labels based on majority class in each cluster
    cluster_0_label = np.bincount(y_test[test_cluster_labels == 0]).argmax()
    cluster_1_label = np.bincount(y_test[test_cluster_labels == 1]).argmax()
    predicted_labels = np.where(test_cluster_labels == 0, cluster_0_label, cluster_1_label)

    # Calculate accuracy
    accuracy = accuracy_score(y_test, predicted_labels)
    accuracies_agglomerative.append(accuracy)

# Average accuracy across folds
avg_accuracy_agglomerative = np.mean(accuracies_agglomerative)
print("Average Cross-Validation Accuracy with Agglomerative Clustering:", avg_accuracy_agglomerative)

import pandas as pd
import numpy as np
from sklearn.cluster import SpectralClustering
from sklearn.metrics import accuracy_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import StratifiedKFold
from sklearn.compose import ColumnTransformer
from sklearn.metrics.pairwise import rbf_kernel  # For RBF kernel

# Load dataset from a CSV file (replace 'heart.csv' with the actual file path)
df = pd.read_csv('HeartDiseaseTrain-Test.csv')

# Replace target values based on your logic (if target values need encoding)
df['target'] = df['target'].replace({0: 0, 1: 1, 2: 1, 3: 1, 4: 1})

# Separate features (X) and target (y)
X = df.drop(columns=['target'])
y = df['target']

# Initialize Stratified K-Fold
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Convert y to a numpy array for indexing
y_np = y.to_numpy()

# Identify numerical and categorical features
numerical_features = X.select_dtypes(include=np.number).columns.tolist()
categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()

# Create a ColumnTransformer to handle numerical and categorical features separately
preprocessor = ColumnTransformer(
    transformers=[
        ('num', SimpleImputer(strategy='mean'), numerical_features),
        ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), categorical_features),
    ])

# Perform cross-validation for Spectral Clustering
accuracies_spectral = []
for train_index, test_index in skf.split(X, y):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y_np[train_index], y_np[test_index]

    # Preprocess the data using the ColumnTransformer
    X_train_preprocessed = preprocessor.fit_transform(X_train)
    X_test_preprocessed = preprocessor.transform(X_test)

    # Scale the numerical features (after imputation and one-hot encoding)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_preprocessed)
    X_test_scaled = scaler.transform(X_test_preprocessed)

    # Calculate the similarity matrix (RBF Kernel)
    similarity_matrix = rbf_kernel(X_train_scaled)

    # Initialize Spectral Clustering
    spectral = SpectralClustering(n_clusters=2, affinity='precomputed', random_state=42)

    # Fit Spectral Clustering on the similarity matrix
    spectral_labels = spectral.fit_predict(similarity_matrix)

    # Assign labels based on majority class in each cluster
    cluster_0_label = np.bincount(y_train[spectral_labels == 0]).argmax()
    cluster_1_label = np.bincount(y_train[spectral_labels == 1]).argmax()
    predicted_labels = np.where(spectral_labels == 0, cluster_0_label, cluster_1_label)

    # Calculate accuracy
    accuracy = accuracy_score(y_train, predicted_labels)
    accuracies_spectral.append(accuracy)

# Average accuracy across folds
avg_accuracy_spectral = np.mean(accuracies_spectral)
print("Average Cross-Validation Accuracy with Spectral Clustering:", avg_accuracy_spectral)

from sklearn.mixture import GaussianMixture
from sklearn.metrics import accuracy_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import StratifiedKFold
from sklearn.compose import ColumnTransformer
import numpy as np
import pandas as pd

# Load dataset from a CSV file (replace 'heart.csv' with the actual file path)
df = pd.read_csv('HeartDiseaseTrain-Test.csv')

# Replace target values based on your logic (if target values need encoding)
df['target'] = df['target'].replace({0: 0, 1: 1, 2: 1, 3: 1, 4: 1})

# Separate features (X) and target (y)
X = df.drop(columns=['target'])
y = df['target']

# Initialize Stratified K-Fold
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Convert y to a numpy array for indexing
y_np = y.to_numpy()

# Identify numerical and categorical features
numerical_features = X.select_dtypes(include=np.number).columns.tolist()
categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()

# Create a ColumnTransformer to handle numerical and categorical features separately
preprocessor = ColumnTransformer(
    transformers=[
        ('num', SimpleImputer(strategy='mean'), numerical_features),
        ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), categorical_features),
    ])

# Perform cross-validation for GMM
accuracies_gmm = []
for train_index, test_index in skf.split(X, y):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y_np[train_index], y_np[test_index]

    # Preprocess the data using the ColumnTransformer
    X_train_preprocessed = preprocessor.fit_transform(X_train)
    X_test_preprocessed = preprocessor.transform(X_test)

    # Scale the numerical features (after imputation and one-hot encoding)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_preprocessed)
    X_test_scaled = scaler.transform(X_test_preprocessed)

    # Initialize Gaussian Mixture Model (GMM)
    gmm = GaussianMixture(n_components=2, random_state=42)

    # Fit GMM on the scaled training data
    gmm.fit(X_train_scaled)

    # Predict cluster labels for the testing data
    test_cluster_labels = gmm.predict(X_test_scaled)

    # Assign labels based on majority class in each cluster
    cluster_0_label = np.bincount(y_test[test_cluster_labels == 0]).argmax()
    cluster_1_label = np.bincount(y_test[test_cluster_labels == 1]).argmax()
    predicted_labels = np.where(test_cluster_labels == 0, cluster_0_label, cluster_1_label)

    # Calculate accuracy
    accuracy = accuracy_score(y_test, predicted_labels)
    accuracies_gmm.append(accuracy)

# Average accuracy across folds
avg_accuracy_gmm = np.mean(accuracies_gmm)
print("Average Cross-Validation Accuracy with Gaussian Mixture Model:", avg_accuracy_gmm)

import matplotlib.pyplot as plt

# Define average accuracies for each model (replace these values with actual data)
avg_accuracy = 0.85
avg_accuracy_dbscan = 0.78
avg_accuracy_agglomerative = 0.82
avg_accuracy_spectral = 0.80
avg_accuracy_gmm = 0.83

# Average accuracies for each model (including GMM)
avg_accuracies = {
    "KMeans": avg_accuracy,
    "DBSCAN": avg_accuracy_dbscan,
    "Agglomerative": avg_accuracy_agglomerative,
    "Spectral Clustering": avg_accuracy_spectral,
    "GMM": avg_accuracy_gmm
}

# Plotting the accuracies with a single color for all bars
plt.figure(figsize=(10, 6))
plt.bar(avg_accuracies.keys(), avg_accuracies.values(), color='skyblue')  # Use one color, e.g., 'skyblue'
plt.title('Average Accuracy Comparison Across Clustering Models')
plt.xlabel('Clustering Models')
plt.ylabel('Average Accuracy')
plt.ylim(0, 1)
plt.show()

import matplotlib.pyplot as plt

# Average accuracies for each model (including GMM)
avg_accuracies = {
    "KMeans": avg_accuracy,
    "DBSCAN": avg_accuracy_dbscan,
    "Agglomerative": avg_accuracy_agglomerative,
    "Spectral Clustering": avg_accuracy_spectral,
    "GMM": avg_accuracy_gmm
}

# Create a line plot
plt.figure(figsize=(8, 6))
plt.plot(avg_accuracies.keys(), avg_accuracies.values(), marker='o', color='b', linestyle='-', linewidth=2)

# Set titles and labels
plt.title('Average Accuracy for Different Models')
plt.xlabel('Clustering Model')
plt.ylabel('Average Accuracy')

# Show the plot
plt.ylim(0, 1)  # Set y-axis limits to [0, 1] for accuracy
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns
from sklearn.cluster import SpectralClustering

# Replace these with the actual predicted labels from your models
# Example: predicted_labels_kmeans, predicted_labels_dbscan, etc.
# These are the predicted labels from your clustering models
predicted_labels_kmeans = kmeans.predict(X_test_scaled)
predicted_labels_dbscan = dbscan.fit_predict(X_test_scaled)
predicted_labels_agglomerative = agglomerative.fit_predict(X_test_scaled)

# Use raw feature data for Spectral Clustering
spectral = SpectralClustering(n_clusters=2, affinity='nearest_neighbors', random_state=42)
predicted_labels_spectral = spectral.fit_predict(X_test_scaled)

# GMM predictions
predicted_labels_gmm = gmm.predict(X_test_scaled)

# True labels (y_test) - Ensure these are the correct ground truth labels
true_labels = y_test

# Create confusion matrices for each model
cm_kmeans = confusion_matrix(true_labels, predicted_labels_kmeans)
cm_dbscan = confusion_matrix(true_labels, predicted_labels_dbscan)
cm_agglomerative = confusion_matrix(true_labels, predicted_labels_agglomerative)
cm_spectral = confusion_matrix(true_labels, predicted_labels_spectral)
cm_gmm = confusion_matrix(true_labels, predicted_labels_gmm)

# List of confusion matrices and corresponding model names
cm_list = [cm_kmeans, cm_dbscan, cm_agglomerative, cm_spectral, cm_gmm]
model_names = ['KMeans', 'DBSCAN', 'Agglomerative Clustering', 'Spectral Clustering', 'GMM']

# Assume we have feature names in the dataset X_test_scaled.columns (if you're working with a DataFrame)
# If X_test_scaled is a numpy array, we won't have feature names, so this part is optional
feature_names = X_test.columns if hasattr(X_test, 'columns') else [f'Feature {i}' for i in range(X_test_scaled.shape[1])]

# Plotting each confusion matrix separately with feature names in the matrix
for i, cm in enumerate(cm_list):
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=feature_names[:len(cm[0])], yticklabels=feature_names[:len(cm)], cbar=False)
    plt.title(f'Confusion Matrix for {model_names[i]}')
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.show()

import matplotlib.pyplot as plt
from sklearn.metrics import classification_report

# Assuming `y_test` contains the true labels and the models have been trained and predictions made.
# Replace these with actual predicted labels
predicted_labels_kmeans = kmeans.predict(X_test_scaled)
predicted_labels_dbscan = dbscan.fit_predict(X_test_scaled)
predicted_labels_agglomerative = agglomerative.fit_predict(X_test_scaled)
predicted_labels_spectral = spectral.fit_predict(X_test_scaled)
predicted_labels_gmm = gmm.predict(X_test_scaled)

# True labels (y_test) - Ensure these are the correct ground truth labels
true_labels = y_test

# Classification report for each model (with zero_division=0 to avoid warning)
report_kmeans = classification_report(true_labels, predicted_labels_kmeans, zero_division=0)
report_dbscan = classification_report(true_labels, predicted_labels_dbscan, zero_division=0)
report_agglomerative = classification_report(true_labels, predicted_labels_agglomerative, zero_division=0)
report_spectral = classification_report(true_labels, predicted_labels_spectral, zero_division=0)
report_gmm = classification_report(true_labels, predicted_labels_gmm, zero_division=0)

# Print classification reports
print("KMeans Classification Report:")
print(report_kmeans)

print("\nDBSCAN Classification Report:")
print(report_dbscan)

print("\nAgglomerative Clustering Classification Report:")
print(report_agglomerative)

print("\nSpectral Clustering Classification Report:")
print(report_spectral)

print("\nGMM Classification Report:")
print(report_gmm)

# Average accuracies for each model (including GMM)
avg_accuracies = {
    "KMeans": avg_accuracy_kmeans,
    "DBSCAN": avg_accuracy_dbscan,
    "Agglomerative": avg_accuracy_agglomerative,
    "Spectral Clustering": avg_accuracy_spectral,
    "GMM": avg_accuracy_gmm
}

# Print average accuracies
print("\nAverage Accuracies for each model:")
for model_name, accuracy in avg_accuracies.items():
    print(f"{model_name}: {accuracy:.4f}")

import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics import accuracy_score, silhouette_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import StratifiedKFold
from sklearn.compose import ColumnTransformer
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load dataset from a CSV file (replace 'HeartDiseaseTrain-Test.csv' with the actual file path)
df = pd.read_csv('HeartDiseaseTrain-Test.csv')

# Replace target values based on your logic (if target values need encoding)
df['target'] = df['target'].replace({0: 0, 1: 1, 2: 1, 3: 1, 4: 1})

# Separate features (X) and target (y)
X = df.drop(columns=['target'])  # Assuming 'target' is the column name for the labels
y = df['target']

# Initialize Stratified K-Fold
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Convert y to a numpy array for indexing
y_np = y.to_numpy()

# Identify numerical and categorical features
numerical_features = X.select_dtypes(include=np.number).columns.tolist()
categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()

# Create a ColumnTransformer to handle numerical and categorical features separately
preprocessor = ColumnTransformer(
    transformers=[
        ('num', SimpleImputer(strategy='mean'), numerical_features),
        ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), categorical_features) #sparse=False for numpy array
    ])

# Determine optimal number of clusters using the Elbow Method and Silhouette Score
inertia = []
sil_scores = []
for k in range(2, 11):  # Test from 2 to 10 clusters
    kmeans = KMeans(n_clusters=k, init='k-means++', n_init=50, max_iter=500, random_state=42)

    # Preprocess the entire dataset (both numerical and categorical)
    X_preprocessed = preprocessor.fit_transform(X)

    # Fit the model
    kmeans.fit(X_preprocessed)

    inertia.append(kmeans.inertia_)
    sil_score = silhouette_score(X_preprocessed, kmeans.labels_)
    sil_scores.append(sil_score)

# Plot the Elbow Curve and Silhouette Scores
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(range(2, 11), inertia, marker='o')
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')

plt.subplot(1, 2, 2)
plt.plot(range(2, 11), sil_scores, marker='o', color='r')
plt.title('Silhouette Score for Each k')
plt.xlabel('Number of Clusters')
plt.ylabel('Silhouette Score')

plt.tight_layout()
plt.show()

# Perform cross-validation with improved KMeans model
accuracies = []
for train_index, test_index in skf.split(X, y):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y_np[train_index], y_np[test_index]

    # Preprocess and scale the data
    X_train_preprocessed = preprocessor.fit_transform(X_train)
    X_test_preprocessed = preprocessor.transform(X_test)

    # Fit the StandardScaler on the training set and then transform both train and test sets
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_preprocessed)
    X_test_scaled = scaler.transform(X_test_preprocessed)

    # Apply PCA to reduce dimensionality
    pca = PCA(n_components=0.95)  # Retain 95% variance
    X_train_pca = pca.fit_transform(X_train_scaled)
    X_test_pca = pca.transform(X_test_scaled)

    # Initialize KMeans with 2 clusters (based on Elbow/Silhouette results)
    kmeans = KMeans(n_clusters=2, init='k-means++', n_init=50, max_iter=500, random_state=42)

    # Fit KMeans on the scaled and PCA-reduced training data
    kmeans.fit(X_train_pca)

    # Predict cluster labels for the testing data
    test_cluster_labels = kmeans.predict(X_test_pca)

    # Assign labels based on majority class in each cluster
    cluster_0_label = np.bincount(y_test[test_cluster_labels == 0]).argmax()
    cluster_1_label = np.bincount(y_test[test_cluster_labels == 1]).argmax()
    predicted_labels = np.where(test_cluster_labels == 0, cluster_0_label, cluster_1_label)

    # Calculate accuracy for this fold
    accuracy = accuracy_score(y_test, predicted_labels)
    accuracies.append(accuracy)

# Average accuracy across folds
avg_accuracy = np.mean(accuracies)
print("Average Cross-Validation Accuracy with Improved KMeans:", avg_accuracy)

!pip install hdbscan

import pandas as pd
import numpy as np
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import StratifiedKFold, ParameterGrid
from sklearn.compose import ColumnTransformer
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score
import hdbscan

# Load dataset
df = pd.read_csv('HeartDiseaseTrain-Test.csv')

# Replace target values
df['target'] = df['target'].replace({0: 0, 1: 1, 2: 1, 3: 1, 4: 1})

# Separate features and target
X = df.drop(columns=['target'])
y = df['target']

# Stratified K-Fold
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Convert y to a numpy array
y_np = y.to_numpy()

# Identify numerical and categorical features
numerical_features = X.select_dtypes(include=np.number).columns.tolist()
categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()

# Preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ('num', SimpleImputer(strategy='mean'), numerical_features),
        ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), categorical_features),
    ])

# Extended hyperparameter grid for HDBSCAN
param_grid = {
    'min_cluster_size': [5, 10, 15],
    'min_samples': [1, 5, 10],
    'cluster_selection_method': ['eom', 'leaf']
}

# Cross-validation
best_accuracy = 0

for train_index, test_index in skf.split(X, y):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y_np[train_index], y_np[test_index]

    # Preprocess the data
    X_train_preprocessed = preprocessor.fit_transform(X_train)
    X_test_preprocessed = preprocessor.transform(X_test)

    # Scale the data
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_preprocessed)
    X_test_scaled = scaler.transform(X_test_preprocessed)

    # PCA
    pca = PCA(n_components=0.95)
    X_train_pca = pca.fit_transform(X_train_scaled)
    X_test_pca = pca.transform(X_test_scaled)

    # Hyperparameter tuning for HDBSCAN
    for params in ParameterGrid(param_grid):
        # Fit HDBSCAN
        hdbscan_model = hdbscan.HDBSCAN(**params)
        hdbscan_labels = hdbscan_model.fit_predict(X_train_pca)

        # Skip if all labels are noise
        if np.all(hdbscan_labels == -1):
            continue

        # Combine labels from other clustering methods
        kmeans = KMeans(n_clusters=2, random_state=42)
        gmm = GaussianMixture(n_components=2, random_state=42)
        agglomerative = AgglomerativeClustering(n_clusters=2)

        kmeans_labels = kmeans.fit_predict(X_train_pca)
        gmm_labels = gmm.fit_predict(X_train_pca)
        agglomerative_labels = agglomerative.fit_predict(X_train_pca)

        combined_labels = np.array([
            np.argmax(np.bincount([
                kmeans_labels[i],
                gmm_labels[i],
                agglomerative_labels[i],
                hdbscan_labels[i] if hdbscan_labels[i] != -1 else kmeans_labels[i]
            ]))
            for i in range(len(kmeans_labels))
        ])

        # Calculate accuracy
        accuracy = accuracy_score(y_train, combined_labels)

        # Track best accuracy
        if accuracy > best_accuracy:
            best_accuracy = accuracy

# Final evaluation on test set using best parameters
hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=5)  # Best params based on prior runs
hdbscan_labels = hdbscan_model.fit_predict(X_train_pca)

# Combine labels for the test set
combined_test_labels = np.array([
    np.argmax(np.bincount([
        kmeans.predict(X_test_pca)[i],
        gmm.predict(X_test_pca)[i],
        agglomerative.fit_predict(X_test_pca)[i],
        hdbscan_model.fit_predict(X_test_pca)[i] if hdbscan_model.fit_predict(X_test_pca)[i] != -1 else kmeans.predict(X_test_pca)[i]
    ]))
    for i in range(len(X_test_pca))
])

final_accuracy = accuracy_score(y_test, combined_test_labels)

# Print only the final accuracy
print("Final Test Set Accuracy:", final_accuracy)